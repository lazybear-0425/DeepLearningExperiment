{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - L1 & L2 的差別\n",
    "    - [LINK](https://discuss.pytorch.org/t/how-is-your-experience-of-using-l1-regularization/153565)\n",
    " - 注意tensor和scalar<font color=yellow>不要混用</font>\n",
    " - 備註\n",
    "    - 我沒有切Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "lambd = 1\n",
    "reg = 'L2' # None, 'L1', 'L2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FashionMNIST('data', train=True, transform=ToTensor()\n",
    "                          , download=True)\n",
    "test_data = FashionMNIST('data', train=False, transform=ToTensor()\n",
    "                          , download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.LazyLinear(10)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Module()\n",
    "optimizer = Adam(model.parameters(), lr=0.003)\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'acc':[], 'val_acc':[], 'loss':[], 'val_loss':[]}\n",
    "for i in range(epochs):\n",
    "    print(f'==== Epoch {i + 1} start! ============================')\n",
    "    model.train()\n",
    "    training_acc = 0\n",
    "    training_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        img, label = data\n",
    "        output = model(img)\n",
    "        output_loss = loss(output, label.long())\n",
    "        # record\n",
    "        training_loss = training_loss + output_loss.item() # 因為原本是tensor\n",
    "        training_acc = training_acc + (output.argmax(dim=1) == label).sum().item() # 原本是tensor\n",
    "        # Regularization\n",
    "        # reg_loss = torch.tensor(0, dtype=torch.float32)\n",
    "        # if reg == 'L1': # L1 Norm\n",
    "        #     reg_loss = lambd * sum(param.norm(1) for param in model.parameters())\n",
    "        # elif reg == 'L2': # L2 Norm\n",
    "        #     reg_loss = lambd * sum(param.norm(2) for param in model.parameters())\n",
    "        # output_loss = output_loss + reg_loss.detach()\n",
    "        \n",
    "        if reg == 'L1':\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            output_loss += lambd * l1_norm\n",
    "        elif reg == 'L2':\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            output_loss += lambd * l2_norm\n",
    "\n",
    "        # BP\n",
    "        output_loss.backward()\n",
    "        optimizer.step()\n",
    "    history['loss'].append(training_loss / len(train_data))\n",
    "    history['acc'].append(training_acc / len(train_data))\n",
    "    model.eval()\n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            img, label = data\n",
    "            output = model(img)\n",
    "            test_acc = test_acc + (output.argmax(dim=1) == label).sum().item()\n",
    "            test_loss = test_loss + loss(output, label).item()\n",
    "    history['val_acc'].append(test_acc / len(test_data))\n",
    "    history['val_loss'].append(test_loss / len(test_data))\n",
    "    print(f'Training Accuracy {training_acc / len(train_data):.4f}, training loss: {training_loss / len(train_data):.4f}')\n",
    "    print(f'Test Accuracy: {test_acc / len(test_data):.4f}, test loss: {test_loss / len(test_data):.4f}')\n",
    "    print('===================== End ========================')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf(); plt.cla() # clean\n",
    "plt.plot(history['acc'], 'b', label='training acc')\n",
    "plt.plot(history['val_acc'], 'r', label='val acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf(); plt.cla() # clean\n",
    "plt.plot(history['loss'], 'b', label='training loss')\n",
    "plt.plot(history['val_loss'], 'r', label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = []\n",
    "pred_label = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        output = model(img).argmax(1)\n",
    "        true_label.extend(label.numpy())\n",
    "        pred_label.extend(output.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\033[33m' + 'Accuracy' + '\\033[0m', accuracy_score(true_label, pred_label))\n",
    "print('\\033[33m' + 'Classification Report' + '\\033[0m',\n",
    "      classification_report(true_label, pred_label, target_names=test_data.classes), \n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(true_label, pred_label)\n",
    "plt.cla(); plt.clf()\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=test_data.classes, yticklabels=test_data.classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Fashion-MNIST Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (784, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
